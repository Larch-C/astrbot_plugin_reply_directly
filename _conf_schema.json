    # --- 功能1: 沉浸式对话 (Sticky Reply) ---

    @filter.llm_tool(name="enable_direct_reply_once")
    async def enable_direct_reply_once(self, event: AstrMessageEvent):
        """
        让机器人下次主动回复一次，无需@。仅在需要开启沉浸式对话，引导用户继续对话时使用。
        调用此函数后，机器人会记住当前会话，并在下一次该用户/群组发言时主动响应。
        """
        if not self.config.get("enable_plugin", True):
            return
        
        sticky_config = self.config.get("sticky_reply", {})
        if not sticky_config.get("enable", True):
            return

        session_id = event.unified_msg_origin
        if session_id:
            self.sticky_reply_targets.add(session_id)
            logger.info(f"[沉浸式对话] 已为会话 {session_id} 启用一次性主动回复。")
        
        # 根据需求，此函数不产生任何可见的回复
        # 所以我们不使用 yield event.plain_result()

    @filter.event_message_type(filter.EventMessageType.ALL, priority=100)
    async def sticky_reply_handler(self, event: AstrMessageEvent):
        """
        监听所有消息，检查是否需要执行一次性主动回复。
        高优先级(priority=100)确保它在其他常规处理器之前运行。
        """
        if not self.config.get("enable_plugin", True):
            return
        
        sticky_config = self.config.get("sticky_reply", {})
        if not sticky_config.get("enable", True):
            return
        
        session_id = event.unified_msg_origin
        # 如果当前会话在我们的目标列表中
        if session_id in self.sticky_reply_targets:
            # 立即从目标中移除，确保只回复一次
            self.sticky_reply_targets.remove(session_id)
            logger.info(f"[沉浸式对话] 触发对 {session_id} 的主动回复，消息: '{event.message_str}'")
            
            # 停止事件继续传播，防止其他插件处理或默认的LLM调用（如果它不@机器人）
            event.stop_event()
            
            # 将此消息请求LLM进行处理，并将结果返回给用户
            yield event.request_llm(prompt=event.message_str)

    # --- 功能2: 主动插话 (Proactive Reply) ---
    
    @filter.event_message_type(filter.EventMessageType.GROUP_MESSAGE, priority=999)
    async def record_group_messages(self, event: AstrMessageEvent):
        """
        低优先级监听所有群消息，用于记录聊天历史。
        """
        # 无论功能是否开启，都记录历史，以便随时开启
        group_id = event.get_group_id()
        if not group_id:
            return

        if group_id not in self.group_history:
            self.group_history[group_id] = []
        
        history_limit = self.config.get("proactive_reply", {}).get("history_limit", 10)
        
        # 记录消息和时间戳
        record = (
            event.message_obj.timestamp,
            f"{event.get_sender_name()}: {event.message_str}"
        )
        self.group_history[group_id].append(record)
        
        # 保持历史记录在限制范围内
        self.group_history[group_id] = self.group_history[group_id][-history_limit:]

    @filter.after_message_sent()
    async def proactive_reply_trigger(self, event: AstrMessageEvent):
        """
        当机器人发送消息后触发，启动一个延时任务来检查后续聊天。
        """
        if not self.config.get("enable_plugin", True):
            return
            
        proactive_config = self.config.get("proactive_reply", {})
        if not proactive_config.get("enable", True):
            return
        
        # 此功能仅在群聊中生效
        if event.message_obj.type != MessageType.GROUP_MESSAGE:
            return

        group_id = event.get_group_id()
        if not group_id:
            return

        logger.info(f"[主动插话] 机器人已在群 {group_id} 发言，准备启动延时检查任务。")
        self.bot_last_spoke_time[group_id] = time.time()
        
        # 创建一个异步任务来执行后续检查
        asyncio.create_task(self.check_chat_history(event))

    async def check_chat_history(self, original_event: AstrMessageEvent):
        """
        延时检查机器人在群里说话后的聊天记录，并决定是否插话。
        """
        proactive_config = self.config.get("proactive_reply", {})
        delay = proactive_config.get("delay_seconds", 5)
        
        group_id = original_event.get_group_id()
        if not group_id:
            return
            
        # 获取或创建锁
        if group_id not in self.proactive_check_locks:
            self.proactive_check_locks[group_id] = asyncio.Lock()
        
        async with self.proactive_check_locks[group_id]:
            # 等待设定的延迟时间
            await asyncio.sleep(delay)
            
            trigger_time = self.bot_last_spoke_time.pop(group_id, None)
            if not trigger_time:
                # 如果时间戳已被其他任务处理，则直接返回
                return

            history = self.group_history.get(group_id, [])
            
            # 筛选出机器人发言后的新消息
            recent_messages = [
                msg for ts, msg in history if ts > trigger_time
            ]
            
            if not recent_messages:
                logger.info(f"[主动插话] 在群 {group_id} 的 {delay}s 内无新消息，任务结束。")
                return

            logger.info(f"[主动插话] 在群 {group_id} 收集到 {len(recent_messages)} 条新消息，准备请求LLM判断。")
            
            # 准备LLM请求
            formatted_history = "\n".join(recent_messages)
            system_prompt = (
                "你是一个群聊观察助手。根据以下最近的聊天记录，"
                "请判断机器人是否需要主动插话进行回应、补充或引导话题。"
                "你的回答必须是一个JSON对象，格式如下: "
                '{"should_reply": boolean, "content": "string"}. '
                '如果需要插话，"should_reply"为true，"content"为要说的内容。'
                '如果不需要，"should_reply"为false。'
            )
            
            try:
                # 使用底层API调用LLM，因为它不会触发其他副作用
                llm_provider = self.context.get_using_provider()
                if not llm_provider:
                    logger.warning("[主动插话] 未找到可用的大语言模型提供商。")
                    return
                
                llm_response = await llm_provider.text_chat(
                    prompt=f"聊天记录:\n---\n{formatted_history}\n---",
                    system_prompt=system_prompt,
                    session_id=None, # 不关联任何特定对话
                )

                if llm_response and llm_response.completion_text:
                    logger.debug(f"[主动插話] LLM原始返回: {llm_response.completion_text}")
                    # 解析JSON
                    try:
                        # 尝试从文本中提取JSON块
                        json_str = llm_response.completion_text.strip()
                        if "```json" in json_str:
                           json_str = json_str.split("```json")[1].split("```")[0]
                        
                        data = json.loads(json_str)
                        if data.get("should_reply") and data.get("content"):
                            logger.info(f"[主动插话] LLM决定插话，内容: {data['content']}")
                            # 发送主动消息
                            chain = [Comp.Plain(data['content'])]
                            await self.context.send_message(original_event.unified_msg_origin, chain)
                        else:
                            logger.info("[主动插话] LLM决定不插话。")
                    except (json.JSONDecodeError, KeyError) as e:
                        logger.error(f"[主动插话] 解析LLM返回的JSON失败: {e}\n原始文本: {llm_response.completion_text}")

            except Exception as e:
                logger.error(f"[主动插话] 请求LLM时发生未知错误: {e}")

    async def terminate(self):
        """插件停用或卸载时调用"""
        self.sticky_reply_targets.clear()
        self.group_history.clear()
        self.bot_last_spoke_time.clear()
        logger.info("ReplyDirectlyPlugin 已卸载，资源已清理。")

```

---

### 3. `README.md`

这是一个简单的说明文档，您可以根据需要进行修改和完善。

````markdown
# Reply Directly Plugin for AstrBot

一个为 AstrBot 开发的插件，旨在增强机器人的对话流畅性和主动性。

仓库地址: [https://github.com/qa296/astrbot_plugin_reply_directly](https://github.com/qa296/astrbot_plugin_reply_directly)

## 功能

本插件包含两个核心功能，均可在插件配置中独立开关。

### 1. 沉浸式对话 (Sticky Reply)

- **作用**: 让机器人可以在一次对话中“记住”用户，并在下一次用户发言时，即使没有被`@`，也能主动进行回复。这对于引导多轮对话非常有用。
- **触发方式**: 大语言模型（LLM）在对话中认为需要开启沉浸式对话时，通过调用名为 `enable_direct_reply_once` 的函数工具来激活。
- **效果**: 激活后，机器会等待当前会话（私聊或群聊）的下一次消息，并主动对其进行回复。此效果仅生效一次。

### 2. 主动插话 (Proactive Reply)

- **作用**: 让机器人在群聊中表现得更像一个真实的参与者。在它自己发完言后，会“聆听”一小段时间，如果发现群友的讨论和它刚才的话题相关或有补充的价值，它会主动进行插话。
- **触发方式**: 机器人自己在群里发送消息后自动触发。
- **流程**:
    1. 机器人发言。
    2. 等待 `delay_seconds`（可配置，默认5秒）。
    3. 收集这期间的新聊天记录。
    4. 将记录发送给LLM，让LLM判断是否应该插话以及说什么。
    5. 如果LLM决定插话，机器人会主动发送消息。

## 安装

1.  下载插件包（或通过 `git clone`）。
2.  将 `astrbot_plugin_reply_directly` 文件夹放入 AstrBot 的 `data/plugins/` 目录下。
3.  重启 AstrBot 或在 WebUI 的插件管理页面点击“重载插件列表”。
4.  在插件列表中找到本插件并启用。

## 配置

在 AstrBot WebUI 的插件管理页面，找到本插件并点击“管理”即可看到以下配置项：

-   `插件总开关`: 控制整个插件的启用和禁用。
-   **沉浸式对话设置**:
    -   `开启沉浸式对话功能`: 开启或关闭此功能。
-   **主动插话设置**:
    -   `开启主动插话功能`: 开启或关闭此功能。
    -   `延迟时间（秒）`: 机器人发言后，等待多少秒再检查聊天记录。
    -   `分析历史消息数量`: 最多分析最近多少条消息来决定是否插话。

## 对开发者的提示 (LLM Prompt Engineering)

为了让大语言模型（LLM）能正确使用本插件的功能，您可以在您的 `System Prompt` 中加入以下或类似的描述：

### 沉浸式对话

> 你拥有一个名为 `enable_direct_reply_once` 的函数工具。当你认为需要引导用户进行下一轮对话，或者你的回答需要用户紧接着进行补充说明时，你可以调用此函数。调用后，我（机器人）会在用户下一次发言时主动回复，无需等待用户@我。请注意，这个效果只持续一次。

### 主动插话

> 当你在群聊中发言后，我会观察一小段时间内的后续聊天。我会将这些聊天记录发给你，并让你以JSON格式（`{"should_reply": boolean, "content": "string"}`）判断是否需要主动插话。请根据上下文，做出自然的、有价值的补充或回应。

````

### 使用说明

1.  将上述三个文件放到 `AstrBot/data/plugins/astrbot_plugin_reply_directly/` 目录下。
2.  启动或重载 AstrBot。
3.  在 WebUI 中启用插件并根据需要进行配置。
4.  为了让 LLM 能使用“沉浸式对话”，请确保您的 System Prompt 包含了对 `enable_direct_reply_once` 函数的描述，如 `README.md` 中所示。
